{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTPwb8ny-hOU"
      },
      "source": [
        "**CSE 5522 Lab #2: Sentiment Analysis**\n",
        "\n",
        "The goals of this lab are to familarize you with:\n",
        "\n",
        "*   Naive Bayes\n",
        "*   Binary Classification\n",
        "*   Data exploration\n",
        "*   Working with text-based data (Tweets)\n",
        "\n",
        "**Initial notes**\n",
        "\n",
        "* (If you are using Google Colab) Make a copy of this page in your google drive so that you can edit it.\n",
        "\n",
        "* While not completely necessary for this assignment, you may want to familiarize yourself with the following packages: [numpy](https://numpy.org), [scikit-learn](https://scikit-learn.org), [pandas](https://pandas.pydata.org), [matplotlib](https://matplotlib.org).\n",
        " * Especially numpy, many of the calculations in this (and later) lab can be done in one line using numpy. Whereas raw python may require 5-10x that.\n",
        "\n",
        "* Feel free to (please do!) change the structure of the document below. Especially, add code sections to break your code into logical pieces and add text sections to explain your code or results\n",
        "\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp2T5S8IQTct"
      },
      "source": [
        "**Part 1: A Simple Bayes Net: Naive Bayes**\n",
        "\n",
        "In class, we discussed how conditional independences of a joint probablity distribution get encoded by a Bayesian Network. One of the simplest form of BNs is the Naive Bayes model which encodes a set of simple conditional independences:\n",
        "\n",
        "- Given a single cause all of the effects are independent from each other.\n",
        "- Mathematically:\n",
        "$P($*cause*$, $*effect*$_1, ..., $*effect*$_n) = P($*cause*$) \\prod_i P($*effect*$_i|$*cause*$)$\n",
        "\n",
        "NB can be used for classification by assuming that cause is the true (unknown) label and it (probabilistically) generates all of the features (effects) while features are independent given the cause.\n",
        "\n",
        "For example, in sentiment analysis the *cause* is the author's sentiment (say, unknown label from the set of {sad, happy, feared, suprised, disgusted, angry}) and the *effects* are words that s/he writes. The simplifying assumption of NB says that knowing the latent sentiment, words of the sentence are independent. We know this assumption is not true because grammar and word-use impose some dependency structure between words in the sentence, but we choose to ignore that in this model.\n",
        "\n",
        "Although simple, NB has shown good performance in many classifcation tasks and has become a standard classic baseline for classification.\n",
        "\n",
        "Today we want to perform Twitter sentiment analysis using NB. The goal is to figure out if a tweet has a positive or negative sentiment about the weather.  \n",
        "\n",
        "**1.0:** Set up the environment (you can click on the play button below to import the appropriate modules)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6fYsm5f-UbI"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-MaOhV5UUvD"
      },
      "source": [
        "**1.1** Read the data from GitHub into a pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HskImt85UhU-"
      },
      "source": [
        "TweetUrl='https://github.com/aasiaeet/cse5522data/raw/master/db3_final_clean.csv'\n",
        "tweet_dataframe=pd.read_csv(TweetUrl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rvgt0PYLVHYr"
      },
      "source": [
        "**1.2** Print out the top of the dataframe to make sure that the data loaded correctly.  It should be a data table with three columns (weight, tweet, label), and 3697 rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScERznWSVBK3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "f181fbd7-80ab-46fe-831a-f07c0584ab8a"
      },
      "source": [
        "display(tweet_dataframe.shape)\n",
        "tweet_dataframe.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(3697, 3)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>weight</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>it is very cold out want it to be warmer</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.7698</td>\n",
              "      <td>dammmmmmm its pretty cold this morning   burr lol</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.6146</td>\n",
              "      <td>why does halsey have to be so far away think m...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.9356</td>\n",
              "      <td>dammit stop being so cold so can work out</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>its too freakin cold</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   weight                                              tweet  label\n",
              "0  1.0000          it is very cold out want it to be warmer      -1\n",
              "1  0.7698  dammmmmmm its pretty cold this morning   burr lol     -1\n",
              "2  0.6146  why does halsey have to be so far away think m...     -1\n",
              "3  0.9356         dammit stop being so cold so can work out      -1\n",
              "4  1.0000                               its too freakin cold     -1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLwuLvFQuvGU"
      },
      "source": [
        "Labels are -1 and +1 for negative and positive sentiments respectively. Multiple judges have been asked to choose a label for a tweet (this is an example of crowd-sourcing) from five possible labels:\n",
        "\n",
        "- Tweet is not relevant to weather.\n",
        "- I can't tell the sentiment.\n",
        "- Neutral: author just sharing information.\n",
        "- Positive\n",
        "- Negative\n",
        "\n",
        "The majority vote was picked as the label and its ratio was set as the weight of the tweet. So for the tweet in row 2 above, 61% of judges voted that the label is negative.\n",
        "\n",
        "Note that tweets have been pre-processed (or cleaned). For example, :) and :( :) were replaced with \"sad\" and \"smiley\" and numbers with \"num\", etc. You can go further (as we ask in 1.12) and remove the stop words, i.e., repetitive non-informative words such as am, is, and are."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2AssaBgdR0K"
      },
      "source": [
        "**1.3.** In the next step, we should build our feature matrix by converting the string of words to a vector of numeric values.\n",
        "\n",
        "First we need to assign a unique id to each word and create the feature matrix with correct size:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8Q7tGhlVcOR"
      },
      "source": [
        "# wordDict maps words to id\n",
        "# X is the document-word matrix holding the presence/absence of words in each tweet\n",
        "wordDict = {}\n",
        "idCounter = 0\n",
        "for i in range(tweet_dataframe.shape[0]):\n",
        "  allWords = tweet_dataframe.iloc[i,1].split(\" \")\n",
        "  for word in allWords:\n",
        "    if word not in wordDict:\n",
        "      wordDict[word] = idCounter\n",
        "      idCounter += 1\n",
        "X = np.zeros((tweet_dataframe.shape[0], idCounter),dtype='float')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JkM_oBZ7cv7"
      },
      "source": [
        "Checking head of the dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdK4g_D8hX-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0881e13f-73b3-4600-d7fb-00bd7632ae10"
      },
      "source": [
        "dict(list(wordDict.items())[0:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'': 9,\n",
              " 'be': 7,\n",
              " 'cold': 3,\n",
              " 'is': 1,\n",
              " 'it': 0,\n",
              " 'out': 4,\n",
              " 'to': 6,\n",
              " 'very': 2,\n",
              " 'want': 5,\n",
              " 'warmer': 8}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFmXPr6qeSQo"
      },
      "source": [
        "**1.4:** The simplest way of coding a tweet to numbers is to mark the occurrence of a word, and forget about its frequency in the document (tweet). This works well with tweets as there are not many repetitive words in a single tweet. So let's fill the document-word matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap5o8fzI7rgQ"
      },
      "source": [
        "for i in range(tweet_dataframe.shape[0]):\n",
        "  allWords = tweet_dataframe.iloc[i,1].split(\" \")\n",
        "  for word in allWords:\n",
        "    X[i, wordDict[word]]  = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "714iX2JA9XMh"
      },
      "source": [
        "Now we check if the number of words are correct:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aiap5wBW86lZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eb59a06-9f71-4564-c1b4-39e15aaa030d"
      },
      "source": [
        "np.sum(X[0:5, ], axis = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([10.,  9., 17.,  9.,  4.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmPNK1Su-Hwf"
      },
      "source": [
        "Finally, we extract the labels from the dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGm_x8Nm-HL6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba9feabc-bbfe-48d9-d2bd-10fadc19b340"
      },
      "source": [
        "y = np.array(tweet_dataframe.iloc[:,2])\n",
        "y[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1, -1, -1, -1, -1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ-EgzBo-wLd"
      },
      "source": [
        "Let's compute the total number of positive and negative tweets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFKKNsM7-_UN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "76c3fe12-c51d-4f24-d6bc-5e05fb03bd4c"
      },
      "source": [
        "numNeg = np.sum(y<0)\n",
        "numPos = np.sum(y>=0) #len(y) - numNeg\n",
        "probNeg = numNeg / (numNeg + numPos)\n",
        "probPos = 1 - probNeg\n",
        "display(numNeg, numPos, probNeg, probPos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1650"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "2047"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.4463078171490398"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.5536921828509602"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlOIWDiI7JQw"
      },
      "source": [
        "So samples 0:1649 are negative and 1650:-1 are positive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-s3N0xJOILK"
      },
      "source": [
        "**1.5: Train/Test Split** Now with do the 20/80 split and learn the word probabilities using the 80 % part and test the NB performance on the 20 % part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbdLXcY0PCQX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "af763432-3a8b-4e59-c9ab-7052b762a58c"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "display(xTrain.shape, xTest.shape, yTrain.shape, yTest.shape)\n",
        "#Note: random_state=0 fixes the random seed so we get the same split every run. Don't use this below"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(2957, 5989)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(740, 5989)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(2957,)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(740,)"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2LTz2CP95GT"
      },
      "source": [
        "**1.6: Computing Probabilities by Counting** Now the real work begins. Write the code that, from the train feature matrix xTrain computes the needed word probabilites, i.e., $P(word|label)$ where label is + or - and word is any of the words saved in the `wordDict`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ7KjLPk7oMZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "6e62e661-7799-495f-bc6b-905cd630cc5f"
      },
      "source": [
        "# compute three distributions (four variables):\n",
        "\n",
        "#The slow way\n",
        "def compute_distros(x,y):\n",
        "  count_positive=0\n",
        "  for tweetIndex in range(x.shape[0]):\n",
        "    if y[tweetIndex]>0:\n",
        "      count_positive+=1\n",
        "\n",
        "  probWordGivenPositive=np.zeros((x.shape[1]))\n",
        "  probWordGivenNegative=np.zeros((x.shape[1]))\n",
        "  for wordIndex in range(x.shape[1]): #Go through each word and estimate it's distributions\n",
        "    count_present_positive=0\n",
        "    count_present_negative=0\n",
        "    for tweetIndex in range(x.shape[0]):\n",
        "      #Go through each tweet and count depending on positive vs. negative\n",
        "      if x[tweetIndex,wordIndex]>0:\n",
        "        if y[tweetIndex]>=0:\n",
        "          count_present_positive+=1\n",
        "        else:\n",
        "          count_present_negative+=1\n",
        "      probWordGivenPositive[wordIndex]=count_present_positive/count_positive # Present-positive vs. total positive\n",
        "      probWordGivenNegative[wordIndex]=count_present_negative/(x.shape[0]-count_positive) # Present-negative vs. total negative\n",
        "\n",
        "  priorPositive=count_positive/x.shape[0]\n",
        "  priorNegative=1-priorPositive\n",
        "\n",
        "  return probWordGivenPositive, probWordGivenNegative, priorPositive, priorNegative\n",
        "\n",
        "#The fast way\n",
        "def compute_distros(x,y):\n",
        "  # probWordGivenPositive: P(word|Sentiment = +ive)\n",
        "  probWordGivenPositive=np.mean(x[y>=0,:],axis=0) #Sum each word (column) to count how many times each word shows up (in positive examples)\n",
        "                                                  #and Divide by total number of (positive) examples to give distribution\n",
        "\n",
        "  # probWordGivenNegative: P(word|Sentiment = -ive)\n",
        "  probWordGivenNegative=np.mean(x[y<0,:],axis=0)\n",
        "\n",
        "  # priorPositive: P(Sentiment = +ive)\n",
        "  priorPositive = np.mean(y>=0) #Number of positive examples vs. all examples\n",
        "  # priorNegative: P(Sentiment = -ive)\n",
        "  priorNegative = 1 - priorPositive\n",
        "  #  (note these last two form one distribution)\n",
        "  return probWordGivenPositive, probWordGivenNegative, priorPositive, priorNegative\n",
        "\n",
        "# compute distributions here\n",
        "probWordGivenPositive, probWordGivenNegative, priorPositive, priorNegative = compute_distros(xTrain,yTrain)\n",
        "\n",
        "# checking the results\n",
        "display(probWordGivenPositive[0:5])\n",
        "display(probWordGivenNegative[0:5])\n",
        "display(priorPositive, priorNegative)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([0.1185006 , 0.20737606, 0.01088271, 0.01451028, 0.10217654])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([0.14504988, 0.19493477, 0.00537222, 0.09669992, 0.13967767])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.5593506932702063"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.44064930672979374"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUFU8eUQ_8gC"
      },
      "source": [
        "Note that you only needed to compute $P(word = 1| +)$ or $P(word = 1| -)$ and the probabilities of the word being absent from a tweet is just 1 minus those probabilities.\n",
        "\n",
        "However, as we see in 1.7, for convenience, we will also want to compute $log P(word = 1 | +)$, $log P(word = 0 | +)$, $log P(word = 1 | -)$ and $log P(word = 0 | -)$.  Also we should compute the log priors.  Let's do so now.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HLcaaDTiwF0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "9b63b32e-f121-4348-b3dc-64c55c0d1d07"
      },
      "source": [
        "# compute the following:\n",
        "# logProbWordPresentGivenPositive\n",
        "# logProbWordAbsentGivenPositive\n",
        "# logProbWordPresentGivenNegative\n",
        "# logProbWordAbsentGivenNegative\n",
        "# logPriorPositive\n",
        "# logPriorNegative\n",
        "def compute_logdistros(distros, min_prob):\n",
        "  if True:\n",
        "    #Assume missing words are simply very rare\n",
        "    #So, assign minimum probability to very small elements (e.g. 0 elements)\n",
        "    distros=np.where(distros>=min_prob,distros,min_prob)\n",
        "    #Also need to consider minimum probability for \"not\" distribution\n",
        "    distros=np.where(distros<=(1-min_prob),distros,1-min_prob)\n",
        "\n",
        "    return np.log(distros), np.log(1-distros)\n",
        "  else:\n",
        "    #Ignore missing words (assume they have P==1, i.e. force log 0 to 0)\n",
        "    return np.log(np.where(distros>0,distros,1)), np.log(np.where(distros<1,1-distros,1))\n",
        "\n",
        "min_prob = 1/yTrain.shape[0] #Assume very rare words only appeared once\n",
        "logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive = compute_logdistros(probWordGivenPositive,min_prob)\n",
        "logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative = compute_logdistros(probWordGivenNegative,min_prob)\n",
        "logPriorPositive, logPriorNegative = compute_logdistros(priorPositive,min_prob)\n",
        "\n",
        "# Did this work, or did you get an error?  (Read below.)\n",
        "display(logProbWordPresentGivenPositive[0:5])\n",
        "display(logProbWordAbsentGivenPositive[0:5])\n",
        "display(logProbWordPresentGivenNegative[0:5])\n",
        "display(logProbWordAbsentGivenNegative[0:5])\n",
        "display(logPriorPositive, logPriorNegative)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([-2.13283722, -1.57322143, -4.52058012, -4.23289805, -2.28105316])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([-0.12613096, -0.23240639, -0.01094236, -0.01461658, -0.10778182])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([-1.93067756, -1.63509031, -5.22651443, -2.33614267, -1.96841789])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([-0.15671216, -0.21683197, -0.0053867 , -0.10170047, -0.15044815])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "-0.5809786442688406"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "-0.819505942727632"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXVQ7ZHAkH1u"
      },
      "source": [
        "You likely received an error when you tried to take $log(0)$ at some point.  Can your group think of a way to avoid taking $log(0)$?  Check in with your instructor/TA to see if what you're thinking will work.  Implement that change in your code above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxdbYsu9E8av"
      },
      "source": [
        "**1.7: Math of NB** Here we provide the derivation of NB when we want to classify the $i$th tweet $\\textbf{x}^{(i)}$ and the size of dictionary is $p$, i.e., each tweet is a binary vector of size $p$ as $\\textbf{x}^{(i)} = (x_1^{(i)},\\dots, x_p^{(i)})$.\n",
        "\n",
        "Note that we computed $P(x_j^{(i)} = 1|+)$ and $P(x_j^{(i)} = 1|-)$ in above code from `xTrain` and now want to classify `xTest` samples.\n",
        "\n",
        "**Classification Rule:** For each tweet $i$ NB classifier assigns label + if $P(+|\\textbf{x}^{(i)}) > P(-|\\textbf{x}^{(i)})$ and negative otherwise.\n",
        "\n",
        "These posterior probabilities can be computed using prior probabilities (that we got from `xTrain`) and Bayes rule as follows:\n",
        "\n",
        "\\begin{align}\n",
        "P(+|\\textbf{x}^{(i)}) &= \\alpha P(\\{\\textbf{x}^{(i)}\\}_{i=1}^n | +)P(+)\n",
        "\\\\\n",
        "(\\text{NB Assumption}) &= \\alpha P(+) \\prod_{j=1}^p P(x_j^{(i)}|+)\n",
        "\\end{align}\n",
        "\n",
        "For computational convinence (preventing underflow while dealing with small numbers) we work with the $\\log$ of probabilities:\n",
        "\n",
        "\\begin{align}\n",
        "\\log(P(+|\\textbf{x}^{(i)})) &\\propto \\log P(+) + \\sum_{j=1}^p \\log P(x_j^{(i)}|+)\n",
        "\\\\\n",
        "\\log(P(-|\\textbf{x}^{(i)})) &\\propto \\log P(-) + \\sum_{j=1}^p \\log P(x_j^{(i)}|-)\n",
        "\\end{align}\n",
        "\n",
        "Finally we can compute the confidence of our prediction as the log of the ratio of posteriors:\n",
        "$\\log(\\frac{P(\\text{predicted label}|\\textbf{x}^{(i)})}{P(\\text{the other label}|\\textbf{x}^{(i)})})$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1AvG-LXTmPJ"
      },
      "source": [
        "**1.8: Implementing NB** Now write a function that takes a row of `xTest` and output a label for it based on NB classification rule.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1ni9dI0egul"
      },
      "source": [
        "def logSignGivenTweet(words, logWordsPresentGivenSign, logWordsAbsentGivenSign, logPriorSign):\n",
        "  temp = words.copy()\n",
        "  for wordIndex, word in enumerate(temp):\n",
        "    if word == 0:\n",
        "      temp[wordIndex] = logWordsAbsentGivenSign[wordIndex]\n",
        "    else:\n",
        "      temp[wordIndex] = logWordsPresentGivenSign[wordIndex]\n",
        "  result = sum(temp) + logPriorSign\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu3YKPlzeFLb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2d52326-15e7-4551-b6d8-803746fb14ed"
      },
      "source": [
        "# classifyNB:\n",
        "#   words - vector of words of the tweet (binary vector)\n",
        "#   logProbWordPresentGivenPositive - log P(x_j = 1|+)\n",
        "#   logProbWordAbsentGivenPositive  - log P(x_j = 0|+)\n",
        "#   logProbWordPresentGivenNegative - log P(x_j = 1|-)\n",
        "#   logProbWordAbsentGivenNegative  - log P(x_j = 0|-)\n",
        "#   logPriorPositive - log P(+)\n",
        "#   logPriorNegative - log P(-)\n",
        "#   returns (label of x according to the NB classification rule, confidence about the label)\n",
        "\n",
        "# Note: you can also change the function definition if you wish to encapsulate all six log probs\n",
        "# as one model; just make sure to follow through below\n",
        "\n",
        "def classifyNB(words,logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive,\n",
        "               logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative,\n",
        "               logPriorPositive, logPriorNegative):\n",
        "  # fill in function definition here\n",
        "  logPositiveGivenTweet = logSignGivenTweet(words, logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive, logPriorPositive)\n",
        "  logNegativeGivenTweet = logSignGivenTweet(words, logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative, logPriorNegative)\n",
        "  prediction = [np.exp(logNegativeGivenTweet), np.exp(logPositiveGivenTweet)]\n",
        "  if(prediction.index(max(prediction)) == 0):\n",
        "    label = -1\n",
        "  else:\n",
        "    label = 1\n",
        "  confidence = np.log(max(prediction)/min(prediction))\n",
        "\n",
        "  return (label, confidence)\n",
        "print(classifyNB(xTest[700, ], logProbWordPresentGivenPositive,logProbWordAbsentGivenPositive,\n",
        "                               logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative,\n",
        "                               logPriorPositive, logPriorNegative))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 4.37706070095421)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev7F8osLy3ia"
      },
      "source": [
        "**1.9:** Compute the output of `classifyNB` for all test data and output the average error.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_J3BdyCfCVL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c289111b-c180-4cb9-b8d7-c6d30cf44b66"
      },
      "source": [
        "# testNB: Classify all xTest\n",
        "#   xTest - test data features\n",
        "#   yTest - true label of test data\n",
        "#   logProbWordPresentGivenPositive - log P(x_j = 1|+)\n",
        "#   logProbWordAbsentGivenPositive  - log P(x_j = 0|+)\n",
        "#   logProbWordPresentGivenNegative - log P(x_j = 1|-)\n",
        "#   logProbWordAbsentGivenNegative  - log P(x_j = 0|-)\n",
        "#   logPriorPositive - log P(+)\n",
        "#   logPriorNegative - log P(-)\n",
        "#   returns Average test error\n",
        "def testNB(xTest, yTest,\n",
        "           logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive,\n",
        "           logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative,\n",
        "           logPriorPositive, logPriorNegative):\n",
        "  correctCases = 0\n",
        "  for tweetIndex, words in enumerate(xTest):\n",
        "    prediction = classifyNB(words, logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive,\n",
        "           logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative,\n",
        "           logPriorPositive, logPriorNegative)\n",
        "    realResult = yTest[tweetIndex]\n",
        "    if prediction[0] == realResult:\n",
        "      correctCases += 1\n",
        "\n",
        "    avgErr = 1 - (correctCases/len(xTest))\n",
        "  print(\"Average error of NB is\", avgErr)\n",
        "  return avgErr\n",
        "\n",
        "testNB(xTest, yTest,\n",
        "       logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive,\n",
        "       logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative,\n",
        "       logPriorPositive, logPriorNegative)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average error of NB is 0.1702702702702703\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1702702702702703"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHJ97nqVz8CW"
      },
      "source": [
        "**1.10:** Now write an outer wrapper that performs 10 train/test splits and compute the mean and standard deviation of the average accuracy across 10 runs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNGxLT9qzOXl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22733e74-883a-416d-8c19-87e16469be3a"
      },
      "source": [
        "# 10 train/test splits\n",
        "tenErrors = np.array([])\n",
        "for _ in range(10):\n",
        "  xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.2)\n",
        "  probWordGivenPositive, probWordGivenNegative, priorPositive, priorNegative = compute_distros(xTrain,yTrain)\n",
        "  min_prob = 1/yTrain.shape[0] #Assume very rare words only appeared once\n",
        "  logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive = compute_logdistros(probWordGivenPositive,min_prob)\n",
        "  logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative = compute_logdistros(probWordGivenNegative,min_prob)\n",
        "  logPriorPositive, logPriorNegative = compute_logdistros(priorPositive,min_prob)\n",
        "  avgErr = testNB(xTest, yTest,\n",
        "       logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive,\n",
        "       logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative,\n",
        "       logPriorPositive, logPriorNegative)\n",
        "  tenErrors = np.append(tenErrors, avgErr)\n",
        "\n",
        "errorMean = np.average(tenErrors)\n",
        "errorStd = np.std(tenErrors)\n",
        "print(\"The mean of the average error: %f\" %(errorMean))\n",
        "print(\"The standard deviation of the average error: %f\" %(errorStd))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average error of NB is 0.16891891891891897\n",
            "Average error of NB is 0.18648648648648647\n",
            "Average error of NB is 0.16081081081081083\n",
            "Average error of NB is 0.18108108108108112\n",
            "Average error of NB is 0.18108108108108112\n",
            "Average error of NB is 0.18108108108108112\n",
            "Average error of NB is 0.16891891891891897\n",
            "Average error of NB is 0.16621621621621618\n",
            "Average error of NB is 0.16756756756756752\n",
            "Average error of NB is 0.1635135135135135\n",
            "[0.16891892 0.18648649 0.16081081 0.18108108 0.18108108 0.18108108\n",
            " 0.16891892 0.16621622 0.16756757 0.16351351]\n",
            "The mean of the average error: 0.172568\n",
            "The standard deviation of the average error: 0.008505\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfdIpfyeuAbX"
      },
      "source": [
        "**Conclusion**\n",
        "\n",
        "The mean is about 0.17 and the standard deviation is about 0.0085."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VChu9SYOPQ6G"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "**Part 2: An Alternate Model (50 pts)**\n",
        "\n",
        "In Part 1, you calculated the probability of a tweet by incorporating both the probability of words present in the tweet $P\\left(x^i_j=1 | +\\right)$ and the probability of words absent from the tweet $P\\left(x^i_j=0 | +\\right)$.\n",
        "\n",
        "Now, modify your code to *only* incorporate the probability of words present in the tweet $P\\left(x^i_j=1 | +\\right)$ (thus ignoring absent words).\n",
        "\n",
        "Compare this to the original approach in Part 1. Follow reasonable experimental procedure and write up an explanation of the results you find.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK9cc3aFpBi4"
      },
      "source": [
        "# Copy your classifyNB() function and modify as specified\n",
        "def logSignGivenTweet_IgnoreAbsent(words, logWordsPresentGivenSign, logPriorSign):\n",
        "  temp = words.copy()\n",
        "  for wordIndex, word in enumerate(temp):\n",
        "    if word != 0:\n",
        "      temp[wordIndex] = logWordsPresentGivenSign[wordIndex]\n",
        "  result = sum(temp) + logPriorSign\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWkneepzxoIX",
        "outputId": "b8525fad-ba49-4340-d96d-128683f84bca"
      },
      "source": [
        "# classifyNB:\n",
        "#   words - vector of words of the tweet (binary vector)\n",
        "#   logProbWordPresentGivenPositive - log P(x_j = 1|+)\n",
        "#   logProbWordAbsentGivenPositive  - log P(x_j = 0|+)\n",
        "#   logProbWordPresentGivenNegative - log P(x_j = 1|-)\n",
        "#   logProbWordAbsentGivenNegative  - log P(x_j = 0|-)\n",
        "#   logPriorPositive - log P(+)\n",
        "#   logPriorNegative - log P(-)\n",
        "#   returns (label of x according to the NB classification rule, confidence about the label)\n",
        "\n",
        "# Note: you can also change the function definition if you wish to encapsulate all six log probs\n",
        "# as one model; just make sure to follow through below\n",
        "\n",
        "def classifyNB_IgnoreAbsent(words,logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive,\n",
        "               logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative,\n",
        "               logPriorPositive, logPriorNegative):\n",
        "  # fill in function definition here\n",
        "  logPositiveGivenTweet = logSignGivenTweet_IgnoreAbsent(words, logProbWordPresentGivenPositive, logPriorPositive)\n",
        "  logNegativeGivenTweet = logSignGivenTweet_IgnoreAbsent(words, logProbWordPresentGivenNegative, logPriorNegative)\n",
        "  prediction = [np.exp(logNegativeGivenTweet), np.exp(logPositiveGivenTweet)]\n",
        "  if(prediction.index(max(prediction)) == 0):\n",
        "    label = -1\n",
        "  else:\n",
        "    label = 1\n",
        "  confidence = np.log(max(prediction)/min(prediction))\n",
        "\n",
        "  return (label, confidence)\n",
        "print(classifyNB(xTest[700, ], logProbWordPresentGivenPositive,logProbWordAbsentGivenPositive,\n",
        "                               logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative,\n",
        "                               logPriorPositive, logPriorNegative))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(-1, 8.194909102228266)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn0W_iZ00Qko"
      },
      "source": [
        "**Note:** The prediction changed to -1, orginally it was 1 without ignoring the absent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoHf8yljy11T",
        "outputId": "a0388f50-9907-4205-8fb1-0c60306ed4d7"
      },
      "source": [
        "# testNB: Classify all xTest\n",
        "#   xTest - test data features\n",
        "#   yTest - true label of test data\n",
        "#   logProbWordPresentGivenPositive - log P(x_j = 1|+)\n",
        "#   logProbWordAbsentGivenPositive  - log P(x_j = 0|+)\n",
        "#   logProbWordPresentGivenNegative - log P(x_j = 1|-)\n",
        "#   logProbWordAbsentGivenNegative  - log P(x_j = 0|-)\n",
        "#   logPriorPositive - log P(+)\n",
        "#   logPriorNegative - log P(-)\n",
        "#   returns Average test error\n",
        "def testNB_IgnoreAbsent(xTest, yTest,\n",
        "           logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive,\n",
        "           logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative,\n",
        "           logPriorPositive, logPriorNegative):\n",
        "  correctCases = 0\n",
        "  for tweetIndex, words in enumerate(xTest):\n",
        "    prediction = classifyNB_IgnoreAbsent(words, logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive,\n",
        "           logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative,\n",
        "           logPriorPositive, logPriorNegative)\n",
        "    realResult = yTest[tweetIndex]\n",
        "    if prediction[0] == realResult:\n",
        "      correctCases += 1\n",
        "\n",
        "    avgErr = 1 - (correctCases/len(xTest))\n",
        "  print(\"Average error of NB is\", avgErr)\n",
        "  return avgErr\n",
        "\n",
        "testNB_IgnoreAbsent(xTest, yTest,\n",
        "       logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive,\n",
        "       logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative,\n",
        "       logPriorPositive, logPriorNegative)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average error of NB is 0.16486486486486485\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.16486486486486485"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7jFXz5GzELY",
        "outputId": "7fec4bdf-899d-4cf4-dd84-2f35e8223818"
      },
      "source": [
        "# 10 train/test splits\n",
        "tenErrors = np.array([])\n",
        "for _ in range(10):\n",
        "  xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.2)\n",
        "  probWordGivenPositive, probWordGivenNegative, priorPositive, priorNegative = compute_distros(xTrain,yTrain)\n",
        "  min_prob = 1/yTrain.shape[0] #Assume very rare words only appeared once\n",
        "  logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive = compute_logdistros(probWordGivenPositive,min_prob)\n",
        "  logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative = compute_logdistros(probWordGivenNegative,min_prob)\n",
        "  logPriorPositive, logPriorNegative = compute_logdistros(priorPositive,min_prob)\n",
        "  avgErr = testNB_IgnoreAbsent(xTest, yTest,\n",
        "       logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive,\n",
        "       logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative,\n",
        "       logPriorPositive, logPriorNegative)\n",
        "  tenErrors = np.append(tenErrors, avgErr)\n",
        "\n",
        "errorMean = np.average(tenErrors)\n",
        "errorStd = np.std(tenErrors)\n",
        "print(\"The mean of the average error: %f\" %(errorMean))\n",
        "print(\"The standard deviation of the average error: %f\" %(errorStd))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average error of NB is 0.17567567567567566\n",
            "Average error of NB is 0.18243243243243246\n",
            "Average error of NB is 0.177027027027027\n",
            "Average error of NB is 0.16756756756756752\n",
            "Average error of NB is 0.17837837837837833\n",
            "Average error of NB is 0.14729729729729735\n",
            "Average error of NB is 0.17297297297297298\n",
            "Average error of NB is 0.1837837837837838\n",
            "Average error of NB is 0.177027027027027\n",
            "Average error of NB is 0.16486486486486485\n",
            "The mean of the average error: 0.172703\n",
            "The standard deviation of the average error: 0.010145\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyNYuPRRzXny"
      },
      "source": [
        "**Conclusion**\n",
        "\n",
        "The mean of the average error is also about 0.17 and the standard deviation of the average error is about 0.010. Comparing with the result which does not ignore the absent words, the average error is about the same. In conclusion, ignoring the absent words does not really affect the accuracy of the prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Khjmiq7QPXS-"
      },
      "source": [
        "---\n",
        "---\n",
        "**Part 3: An Additional Experiment (50 pts)**\n",
        "\n",
        "Implement an experiment to change the model, and report on the results of the experiment, comparing to your baseline model from Part 1 (and your alternate model from Part 2).\n",
        "\n",
        "Choose one (and only one) of the following options.\n",
        "\n",
        "*Please make clear which option you choose! (For example, by deleting the options you are not choosing.)*\n",
        "\n",
        "**3.1: Option 1: Removing stop words**\n",
        "\n",
        "Investigate the effect of removing the 25, 50, 100, and 200 most frequent words from the calculation.\n",
        "\n",
        "**3.2: Option 2: Sample weights (3 bonus points for difficulty)**\n",
        "\n",
        "Recall that the labels for each of our data points/samples came with a weight. This weight was based on the proportion of labelers that agreed on this label, so it serves as a kind of measure of confidence we should have in each data point. That is, a weight near 1 indicates everyone agreed on the same label. Whereas a weight below 0.5 means not even a majority agreed on the chosen label.\n",
        "\n",
        "Devise a method for weighting samples, and use that method to recalculate the probability distributions.  Report the effect of weighting samples on the test set.\n",
        "\n",
        "(Hint: Re-examine part 1.6 and think about how you would change this to make it pay more attention to data points with higher weights.)\n",
        "\n",
        "**3.3: Option 3: Sticky terms (6 bonus points for difficulty)**\n",
        "\n",
        "A \"sticky term\" is two words which are more likely to occur together than independently.\n",
        "\n",
        "You can use \"Pointwise Mutual Information\" (PMI) to determine the stickiness, using: $PMI=\\frac{P(w_1,w_2)}{P(w_1)P(w_2)}$.  For all pairs of <u>adjacent</u> words in the tweet corpus, find the top n pairs according to PMI and add them as additional features in your Naive Bayes Model.\n",
        "\n",
        "Find the top 100, 200, 500 \"sticky terms\" and add these as features to the model.\n",
        "\n",
        "(Note, you cannot use X to calculate the above joint distribution, since the above is about adjacent words. You will have to go back to the raw text.)\n",
        "\n",
        "---\n",
        "\n",
        "Remember, you need to *compare* your chosen option with your previous work. Just writing the code is not sufficient. Follow reasonable experimental procedure and write up a discussion of your results and why you think they turned out that way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdgfTW347u3_"
      },
      "source": [
        "**Option 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sDHdimnqjYS"
      },
      "source": [
        "# Further modify your code based on the option you choose above\n",
        "def removeStopWords(tweet, count, logProbWordPresentGivenPositive, logProbWordPresentGivenNegative, logProbWordAbsentGivenPositive, logProbWordAbsentGivenNegative):\n",
        "  listWordCounts = np.sum(tweet, axis = 0)\n",
        "  #Find the top n number of words without changing the indices\n",
        "  topNWords = np.sort(np.argpartition(listWordCounts, len(listWordCounts) - count)[-count:])\n",
        "  for wordIndex in topNWords:\n",
        "    logProbWordPresentGivenPositive[wordIndex] = 0\n",
        "    logProbWordPresentGivenNegative[wordIndex] = 0\n",
        "    logProbWordAbsentGivenPositive[wordIndex] = 0\n",
        "    logProbWordAbsentGivenNegative[wordIndex] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqGrEDMvcZel"
      },
      "source": [
        "**Top 25 most frequent words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdtIZzFhbPD7",
        "outputId": "21c70787-621f-4d2d-fd3c-7b259e935a28"
      },
      "source": [
        "count = 25\n",
        "tenErrors = np.array([])\n",
        "for _ in range(10):\n",
        "  xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.2)\n",
        "  probWordGivenPositive, probWordGivenNegative, priorPositive, priorNegative = compute_distros(xTrain,yTrain)\n",
        "  min_prob = 1/yTrain.shape[0] #Assume very rare words only appeared once\n",
        "  logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive = compute_logdistros(probWordGivenPositive,min_prob)\n",
        "  logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative = compute_logdistros(probWordGivenNegative,min_prob)\n",
        "  logPriorPositive, logPriorNegative = compute_logdistros(priorPositive,min_prob)\n",
        "  removeStopWords(xTrain,count,logProbWordPresentGivenPositive, logProbWordPresentGivenNegative,\n",
        "                  logProbWordAbsentGivenPositive, logProbWordAbsentGivenNegative)\n",
        "  avgErr = testNB(xTest, yTest,\n",
        "       logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive,\n",
        "       logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative,\n",
        "       logPriorPositive, logPriorNegative)\n",
        "  tenErrors = np.append(tenErrors, avgErr)\n",
        "\n",
        "errorMean = np.average(tenErrors)\n",
        "errorStd = np.std(tenErrors)\n",
        "print(\"The mean of the average error: %f\" %(errorMean))\n",
        "print(\"The standard deviation of the average error: %f\" %(errorStd))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average error of NB is 0.17567567567567566\n",
            "Average error of NB is 0.17972972972972978\n",
            "Average error of NB is 0.16486486486486485\n",
            "Average error of NB is 0.16756756756756752\n",
            "Average error of NB is 0.17837837837837833\n",
            "Average error of NB is 0.17297297297297298\n",
            "Average error of NB is 0.1945945945945946\n",
            "Average error of NB is 0.17972972972972978\n",
            "Average error of NB is 0.18918918918918914\n",
            "Average error of NB is 0.17432432432432432\n",
            "The mean of the average error: 0.177703\n",
            "The standard deviation of the average error: 0.008552\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1zM9wFMeAKj"
      },
      "source": [
        "**Top 50 most frequent words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YH5E0os9eFt1",
        "outputId": "6d0eef8c-8567-47d2-f73e-d86a74530a0e"
      },
      "source": [
        "count = 50\n",
        "tenErrors = np.array([])\n",
        "for _ in range(10):\n",
        "  xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.2)\n",
        "  probWordGivenPositive, probWordGivenNegative, priorPositive, priorNegative = compute_distros(xTrain,yTrain)\n",
        "  min_prob = 1/yTrain.shape[0] #Assume very rare words only appeared once\n",
        "  logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive = compute_logdistros(probWordGivenPositive,min_prob)\n",
        "  logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative = compute_logdistros(probWordGivenNegative,min_prob)\n",
        "  logPriorPositive, logPriorNegative = compute_logdistros(priorPositive,min_prob)\n",
        "  removeStopWords(xTrain,count,logProbWordPresentGivenPositive, logProbWordPresentGivenNegative,\n",
        "                  logProbWordAbsentGivenPositive, logProbWordAbsentGivenNegative)\n",
        "  avgErr = testNB(xTest, yTest,\n",
        "       logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive,\n",
        "       logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative,\n",
        "       logPriorPositive, logPriorNegative)\n",
        "  tenErrors = np.append(tenErrors, avgErr)\n",
        "\n",
        "errorMean = np.average(tenErrors)\n",
        "errorStd = np.std(tenErrors)\n",
        "print(\"The mean of the average error: %f\" %(errorMean))\n",
        "print(\"The standard deviation of the average error: %f\" %(errorStd))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average error of NB is 0.22297297297297303\n",
            "Average error of NB is 0.23108108108108105\n",
            "Average error of NB is 0.22432432432432436\n",
            "Average error of NB is 0.2256756756756757\n",
            "Average error of NB is 0.21216216216216222\n",
            "Average error of NB is 0.21621621621621623\n",
            "Average error of NB is 0.22297297297297303\n",
            "Average error of NB is 0.20270270270270274\n",
            "Average error of NB is 0.21351351351351355\n",
            "Average error of NB is 0.22162162162162158\n",
            "The mean of the average error: 0.219324\n",
            "The standard deviation of the average error: 0.007788\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9S1aezpeILa"
      },
      "source": [
        "**Top 100 most frequent words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jDN3HAxeLyY",
        "outputId": "39e6d752-84de-46e8-eb09-a33dca6f3f18"
      },
      "source": [
        "count = 100\n",
        "tenErrors = np.array([])\n",
        "for _ in range(10):\n",
        "  xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.2)\n",
        "  probWordGivenPositive, probWordGivenNegative, priorPositive, priorNegative = compute_distros(xTrain,yTrain)\n",
        "  min_prob = 1/yTrain.shape[0] #Assume very rare words only appeared once\n",
        "  logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive = compute_logdistros(probWordGivenPositive,min_prob)\n",
        "  logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative = compute_logdistros(probWordGivenNegative,min_prob)\n",
        "  logPriorPositive, logPriorNegative = compute_logdistros(priorPositive,min_prob)\n",
        "  removeStopWords(xTrain,count,logProbWordPresentGivenPositive, logProbWordPresentGivenNegative,\n",
        "                  logProbWordAbsentGivenPositive, logProbWordAbsentGivenNegative)\n",
        "  avgErr = testNB(xTest, yTest,\n",
        "       logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive,\n",
        "       logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative,\n",
        "       logPriorPositive, logPriorNegative)\n",
        "  tenErrors = np.append(tenErrors, avgErr)\n",
        "\n",
        "errorMean = np.average(tenErrors)\n",
        "errorStd = np.std(tenErrors)\n",
        "print(\"The mean of the average error: %f\" %(errorMean))\n",
        "print(\"The standard deviation of the average error: %f\" %(errorStd))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average error of NB is 0.254054054054054\n",
            "Average error of NB is 0.25135135135135134\n",
            "Average error of NB is 0.2635135135135135\n",
            "Average error of NB is 0.26216216216216215\n",
            "Average error of NB is 0.27297297297297296\n",
            "Average error of NB is 0.22972972972972971\n",
            "Average error of NB is 0.27972972972972976\n",
            "Average error of NB is 0.25540540540540535\n",
            "Average error of NB is 0.23378378378378384\n",
            "Average error of NB is 0.27297297297297296\n",
            "The mean of the average error: 0.257568\n",
            "The standard deviation of the average error: 0.015552\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lg6SLjY6eNUu"
      },
      "source": [
        "**Top 200 most frequent words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpf4z-jXeQba",
        "outputId": "266d608c-2eb1-4eff-b302-1030b0e5c59a"
      },
      "source": [
        "count = 200\n",
        "tenErrors = np.array([])\n",
        "for _ in range(10):\n",
        "  xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.2)\n",
        "  probWordGivenPositive, probWordGivenNegative, priorPositive, priorNegative = compute_distros(xTrain,yTrain)\n",
        "  min_prob = 1/yTrain.shape[0] #Assume very rare words only appeared once\n",
        "  logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive = compute_logdistros(probWordGivenPositive,min_prob)\n",
        "  logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative = compute_logdistros(probWordGivenNegative,min_prob)\n",
        "  logPriorPositive, logPriorNegative = compute_logdistros(priorPositive,min_prob)\n",
        "  removeStopWords(xTrain,count,logProbWordPresentGivenPositive, logProbWordPresentGivenNegative,\n",
        "                  logProbWordAbsentGivenPositive, logProbWordAbsentGivenNegative)\n",
        "  avgErr = testNB(xTest, yTest,\n",
        "       logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive,\n",
        "       logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative,\n",
        "       logPriorPositive, logPriorNegative)\n",
        "  tenErrors = np.append(tenErrors, avgErr)\n",
        "\n",
        "errorMean = np.average(tenErrors)\n",
        "errorStd = np.std(tenErrors)\n",
        "print(\"The mean of the average error: %f\" %(errorMean))\n",
        "print(\"The standard deviation of the average error: %f\" %(errorStd))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average error of NB is 0.29729729729729726\n",
            "Average error of NB is 0.29054054054054057\n",
            "Average error of NB is 0.28378378378378377\n",
            "Average error of NB is 0.31351351351351353\n",
            "Average error of NB is 0.2851351351351351\n",
            "Average error of NB is 0.29324324324324325\n",
            "Average error of NB is 0.29324324324324325\n",
            "Average error of NB is 0.2783783783783784\n",
            "Average error of NB is 0.31351351351351353\n",
            "Average error of NB is 0.31756756756756754\n",
            "The mean of the average error: 0.296622\n",
            "The standard deviation of the average error: 0.013035\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSyeyGrHgxgq"
      },
      "source": [
        "**Conclusion**:\n",
        "\n",
        "Removing the top 25 words, the accuracy does not change much because the top 25 words are probably including auxiliary verbs, pronouns, and articles that do not have any effects on the sentiment of those tweets. But if continue removing words, there is a decrease on accuracy of the prediction. Therefore, the sweet spot is probably removing top 25 words."
      ]
    }
  ]
}